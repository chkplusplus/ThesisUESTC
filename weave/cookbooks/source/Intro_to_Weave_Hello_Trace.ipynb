{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZmKBnUE8bGR"
      },
      "source": [
        "# Introduction to Traces\n",
        "\n",
        "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "\n",
        "Weave is a toolkit for developing AI-powered applications.\n",
        "\n",
        "Use Weave traces to capture the inputs, outputs, and internal structure of your Python function automatically to observe and debug LLM applications.\n",
        "\n",
        "When you decorate a function with `@weave.op`, Weave records a rich trace of the function while it runs, including any nested operations or external API calls. Use the trace to to debug, understand, and visualize interactions between your code and LLM models, without leaving your notebook.\n",
        "\n",
        "To get started, complete the prerequisites. Then, define a function decorated with `@weave.op` decorator and run it on an example input to track LLM calls. Weave captures and visualizes the trace automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxECTbwy8cvT"
      },
      "outputs": [],
      "source": [
        "# Ensure your dependencies are installed with:\n",
        "!pip install --quiet jedi openai weave"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujOegcPY8j7z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "#@title Set up your credentials\n",
        "inference_provider = \"W&B Inference\" #@param [\"W&B Inference\", \"OpenAI\"]\n",
        "\n",
        "# Set up your W&B project and credentials\n",
        "os.environ[\"WANDB_ENTITY_PROJECT\"] = input(\"Set up your W&B project (team name/project name): \")\n",
        "os.environ[\"WANDB_API_KEY\"] = getpass.getpass(\"Set up your W&B API key (Find it at https://wandb.ai/authorize): \")\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "if inference_provider == \"OpenAI\":\n",
        "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key (Find it at https://platform.openai.com/api-keys): \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTUHuulv8afx"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import weave\n",
        "import time\n",
        "\n",
        "weave.init(os.environ[\"WANDB_ENTITY_PROJECT\"])\n",
        "\n",
        "@weave.op\n",
        "def preprocess_message(message: str) -> str:\n",
        "    \"\"\"é¢„å¤„ç†ç”¨æˆ·è¾“å…¥\"\"\"\n",
        "    time.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´\n",
        "    processed = message.strip().lower()\n",
        "    return processed\n",
        "\n",
        "@weave.op\n",
        "def setup_client(inference_provider: str) -> tuple:\n",
        "    \"\"\"è®¾ç½®OpenAIå®¢æˆ·ç«¯\"\"\"\n",
        "    time.sleep(0.05)  # æ¨¡æ‹Ÿåˆå§‹åŒ–æ—¶é—´\n",
        "\n",
        "    if inference_provider == \"W&B Inference\":\n",
        "        client = OpenAI(\n",
        "            base_url=\"https://api.inference.wandb.ai/v1\",\n",
        "            api_key=os.environ[\"WANDB_API_KEY\"],\n",
        "            project=os.environ[\"WANDB_ENTITY_PROJECT\"],\n",
        "        )\n",
        "        model_name = \"OpenPipe/Qwen3-14B-Instruct\"\n",
        "    else:\n",
        "        client = OpenAI()\n",
        "        model_name = \"gpt-4.1-nano\"\n",
        "\n",
        "    return client, model_name\n",
        "\n",
        "@weave.op\n",
        "def call_llm(client, model_name: str, messages: list) -> str:\n",
        "    \"\"\"è°ƒç”¨LLM API\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=messages,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "@weave.op\n",
        "def postprocess_response(response: str) -> str:\n",
        "    \"\"\"åå¤„ç†å“åº”\"\"\"\n",
        "    time.sleep(0.02)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´\n",
        "    # æ·»åŠ ä¸€äº›æ ¼å¼åŒ–\n",
        "    formatted = f\"ğŸ¤– {response}\"\n",
        "    return formatted\n",
        "\n",
        "@weave.op\n",
        "def create_completion(message: str, inference_provider: str = \"OpenAI\") -> str:\n",
        "    \"\"\"ä¸»æµç¨‹ - åˆ›å»ºå®Œæ•´çš„AIå“åº”\"\"\"\n",
        "\n",
        "    # æ­¥éª¤1: é¢„å¤„ç†\n",
        "    processed_message = preprocess_message(message)\n",
        "\n",
        "    # æ­¥éª¤2: è®¾ç½®å®¢æˆ·ç«¯\n",
        "    client, model_name = setup_client(inference_provider)\n",
        "\n",
        "    # æ­¥éª¤3: å‡†å¤‡æ¶ˆæ¯\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": processed_message},\n",
        "    ]\n",
        "\n",
        "    # æ­¥éª¤4: è°ƒç”¨LLM\n",
        "    raw_response = call_llm(client, model_name, messages)\n",
        "\n",
        "    # æ­¥éª¤5: åå¤„ç†\n",
        "    final_response = postprocess_response(raw_response)\n",
        "\n",
        "    return final_response\n",
        "\n",
        "# æµ‹è¯•è°ƒç”¨\n",
        "message = \"Tell me a joke.\"\n",
        "result = create_completion(message)\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}